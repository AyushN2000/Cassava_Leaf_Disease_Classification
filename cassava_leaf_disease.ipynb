{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-0.3.2-py3-none-any.whl (244 kB)\n",
      "\u001b[K     |████████████████████████████████| 244 kB 887 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.8.1)\n",
      "Requirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.7/site-packages (from timm) (1.7.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->timm) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->timm) (3.7.4.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->timm) (0.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->timm) (1.18.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->timm) (1.18.5)\n",
      "Requirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.7/site-packages (from timm) (1.7.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (8.0.1)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.3.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../input/fmixrepo/')\n",
    "from fmix import make_low_freq_image, binarise_mask\n",
    "\n",
    "!pip install timm\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, random\n",
    "import joblib, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'fold_num': 5,\n",
    "        'seed': 719,\n",
    "        'model_arch': 'tf_efficientnet_b4_ns',\n",
    "        'img_size': 512,\n",
    "        'epochs': 10,\n",
    "        'train_bs': 16,\n",
    "        'valid_bs': 32,\n",
    "        'T_0': 10,\n",
    "        'lr': 1e-4,\n",
    "        'min_lr': 1e-6,\n",
    "        'weight_decay':1e-6,\n",
    "        'num_workers': 4,\n",
    "        'accum_iter': 2, #for backprop with effectively larger batch size\n",
    "        'verbose_step': 1,\n",
    "        'device': 'cuda:0' if torch.cuda.is_available() else \"cpu\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    return im_bgr[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/cassava-leaf-disease-classification/train.csv\")\n",
    "count = train.label.value_counts().sort_index()\n",
    "plt.bar(count.keys(), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[0]\n",
    "    H = size[1]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, df, data_root, transforms=None,\n",
    "            output_label=True, \n",
    "            one_hot_label=False,\n",
    "            do_fmix=False, \n",
    "            fmix_params={\n",
    "                'alpha': 1., \n",
    "                'decay_power': 3., \n",
    "                'shape': (config['img_size'], config['img_size']),\n",
    "                'max_soft': True, \n",
    "                'reformulate': False},\n",
    "            do_cutmix=False,\n",
    "            cutmix_params={\n",
    "                'alpha': 1}\n",
    "            ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.data_root = data_root\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.do_fmix = do_fmix\n",
    "        self.fmix_params = fmix_params\n",
    "        self.do_cutmix = do_cutmix\n",
    "        self.cutmix_params = cutmix_params\n",
    "        \n",
    "        self.output_label = output_label\n",
    "        self.one_hot_label = one_hot_label\n",
    "        \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "                        \n",
    "            if one_hot_label is True:\n",
    "                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n",
    "                            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "          \n",
    "        img  = get_img(\"{}/{}\".format(self.data_root, self.df.loc[index]['image_id']))\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
    "            with torch.no_grad():\n",
    "                lam = np.clip(np.random.beta(self.fmix_params['alpha'], self.fmix_params['alpha']),0.6,0.7)\n",
    "                \n",
    "                # Make mask, get mean / std\n",
    "                mask = make_low_freq_image(self.fmix_params['decay_power'], self.fmix_params['shape'])\n",
    "                mask = binarise_mask(mask, lam, self.fmix_params['shape'], self.fmix_params['max_soft'])\n",
    "    \n",
    "                fmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
    "                fmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[fmix_ix]['image_id']))\n",
    "\n",
    "                if self.transforms:\n",
    "                    fmix_img = self.transforms(image=fmix_img)['image']\n",
    "\n",
    "                mask_torch = torch.from_numpy(mask)\n",
    "                \n",
    "                # mix image\n",
    "                img = mask_torch*img+(1.-mask_torch)*fmix_img\n",
    "\n",
    "                # mix target\n",
    "                rate = mask.sum()/config['img_size']/config['img_size']\n",
    "                target = rate*target + (1.-rate)*self.labels[fmix_ix]\n",
    "                        \n",
    "        if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
    "            with torch.no_grad():\n",
    "                cmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
    "                cmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[cmix_ix]['image_id']))\n",
    "                \n",
    "                if self.transforms:\n",
    "                    cmix_img = self.transforms(image=cmix_img)['image']\n",
    "                    \n",
    "                lam = np.clip(np.random.beta(self.cutmix_params['alpha'], self.cutmix_params['alpha']),0.3,0.4)\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox((config['img_size'], config['img_size']), lam)\n",
    "\n",
    "                img[:, bbx1:bbx2, bby1:bby2] = cmix_img[:, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "                rate = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (config['img_size'] * config['img_size']))\n",
    "                target = rate*target + (1.-rate)*self.labels[cmix_ix]\n",
    "                                        \n",
    "       \n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "            A.RandomResizedCrop(config['img_size'], config['img_size']),\n",
    "            A.Transpose(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            A.CoarseDropout(p=0.5),\n",
    "            A.Cutout(p=0.5),\n",
    "            A.pytorch.ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "            A.CenterCrop(config['img_size'], config['img_size'], p=1.),\n",
    "            A.Resize(config['img_size'], config['img_size']),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            A.pytorch.ToTensorV2(p=1.0),\n",
    "        ], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, data_root='../input/cassava-leaf-disease-classification/train_images/'):\n",
    "    \n",
    "    from catalyst.data.sampler import BalanceClassSampler\n",
    "    \n",
    "    train_df = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_df = df.loc[val_idx,:].reset_index(drop=True)\n",
    "        \n",
    "    train_ds = CassavaDataset(\n",
    "                            train_df,\n",
    "                            data_root,\n",
    "                            transforms=get_train_transforms(),\n",
    "                            output_label=True,\n",
    "                            one_hot_label=False,\n",
    "                            do_fmix=False,\n",
    "                            do_cutmix=False)\n",
    "    \n",
    "    valid_ds = CassavaDataset(\n",
    "                            valid_df,\n",
    "                            data_root,\n",
    "                            transforms=get_valid_transforms(),\n",
    "                            output_label=True)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                            train_ds,\n",
    "                            batch_size=config['train_bs'],\n",
    "                            pin_memory=False,\n",
    "                            drop_last=False,\n",
    "                            shuffle=True,        \n",
    "                            num_workers=config['num_workers'],\n",
    "                            #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n",
    "    )\n",
    "        \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "                            valid_ds, \n",
    "                            batch_size=config['valid_bs'],\n",
    "                            num_workers=config['num_workers'],\n",
    "                            shuffle=False,\n",
    "                            pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n",
    "    model.train()\n",
    "\n",
    "    t = time.time()\n",
    "    running_loss = None\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "\n",
    "        with autocast():\n",
    "            image_preds = model(imgs)\n",
    "            loss = loss_fn(image_preds, image_labels)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01\n",
    "\n",
    "            if ((step + 1) %  config['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                if scheduler is not None and schd_batch_update:\n",
    "                    scheduler.step()\n",
    "\n",
    "            if ((step + 1) % config['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                pbar.set_description(description)\n",
    "                \n",
    "    if scheduler is not None and not schd_batch_update:\n",
    "        scheduler.step()\n",
    "        \n",
    "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
    "    model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        image_preds = model(imgs)\n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        loss = loss_fn(image_preds, image_labels)\n",
    "        \n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]  \n",
    "\n",
    "        if ((step + 1) % config['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
    "            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "            pbar.set_description(description)\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    print('validation multi-class accuracy = {:.4f}'.format((image_preds_all==image_targets_all).mean()))\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        if schd_loss_update:\n",
    "            scheduler.step(loss_sum/sample_num)\n",
    "        else:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean'):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        lsm = F.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            lsm = lsm * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = -(targets * lsm).sum(-1)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 0 started\n",
      "Training dataset length: 17117 Validation dataset length: 4280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b4_ns-d6313a46.pth\n",
      "epoch 0 loss: 0.4625: 100%|██████████| 1070/1070 [13:48<00:00,  1.29it/s]\n",
      "epoch 0 loss: 0.3729: 100%|██████████| 134/134 [01:15<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.4217: 100%|██████████| 1070/1070 [13:35<00:00,  1.31it/s]\n",
      "epoch 1 loss: 0.3378: 100%|██████████| 134/134 [01:12<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 0.3865:  70%|██████▉   | 747/1070 [09:28<04:00,  1.34it/s]"
     ]
    }
   ],
   "source": [
    "seed_everything(config['seed'])\n",
    "\n",
    "folds = StratifiedKFold(n_splits=config['fold_num'],\n",
    "                        shuffle=True,\n",
    "                        random_state=config['seed']).split(np.arange(train.shape[0]),\n",
    "                        train.label.values)\n",
    "\n",
    "data_root_path = '../input/cassava-leaf-disease-classification/train_images/'\n",
    "for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "    print('Training with {} started'.format(fold))\n",
    "    print(\"Training dataset length: \"+str(len(trn_idx))+\" Validation dataset length: \"+str(len(val_idx)))\n",
    "    train_loader, val_loader = prepare_dataloader(train,\n",
    "                                        trn_idx, \n",
    "                                        val_idx,\n",
    "                                        data_root=data_root_path)\n",
    "\n",
    "    device = torch.device(config['device'])\n",
    "    \n",
    "    model = CassvaImgClassifier(config['model_arch'], train.label.nunique(), pretrained=True).to(device)\n",
    "    scaler = GradScaler()   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=config['T_0'], T_mult=1, eta_min=config['min_lr'], last_epoch=-1)\n",
    "    \n",
    "    loss_tr = nn.CrossEntropyLoss().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler, schd_batch_update=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n",
    "\n",
    "        torch.save(model.state_dict(),'{}_fold_{}_{}'.format(config['model_arch'], fold, epoch))\n",
    "        \n",
    "    del model, optimizer, train_loader, val_loader, scaler, scheduler\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
